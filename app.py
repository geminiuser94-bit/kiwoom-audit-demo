import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from sklearn.ensemble import IsolationForest
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from faker import Faker
import requests
from bs4 import BeautifulSoup
import time
import random
from datetime import datetime, timedelta
import json
import re
from newspaper import Article

# ... (Previous imports) ...

# ... (Inside NewsCrawler class) ...

st.set_page_config(
    page_title="í‚¤ì›€ì¦ê¶Œ ë‚´ë¶€ê°ì‚¬ AI ì‹œìŠ¤í…œ",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for Kiwoom Securities Theme (Pink/Navy)
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR:wght@300;400;500;700&display=swap');
    
    /* Global Font & Colors */
    :root {
        --primary-color: #EB008B; /* Kiwoom Pink */
        --secondary-color: #002060; /* Kiwoom Navy */
        --background-color: #F0F2F6;
        --text-color: #333333;
    }
    
    html, body, [class*="css"] {
        font-family: 'Noto Sans KR', sans-serif;
        color: var(--text-color);
    }
    
    .stApp {
        background-color: var(--background-color);
    }
    
    /* Header Styling */
    .main-header {
        background: linear-gradient(135deg, var(--secondary-color) 0%, #003399 100%);
        padding: 25px 30px;
        border-radius: 15px;
        color: white;
        margin-bottom: 25px;
        box-shadow: 0 10px 20px rgba(0,0,0,0.1);
        display: flex;
        align-items: center;
        justify-content: space-between;
    }
    .main-header h1 {
        color: white !important;
        margin: 0;
        font-size: 2.0rem;
        font-weight: 800;
        letter-spacing: -0.5px;
    }
    .main-header p {
        color: rgba(255,255,255,0.8);
        margin: 5px 0 0 0;
        font-size: 0.95rem;
    }
    
    /* Metric Card Styling */
    div[data-testid="stMetric"] {
        background-color: white;
        padding: 20px;
        border-radius: 12px;
        box-shadow: 0 4px 6px rgba(0,0,0,0.05);
        border: 1px solid #e0e0e0;
        transition: transform 0.2s;
    }
    div[data-testid="stMetric"]:hover {
        transform: translateY(-2px);
        border-color: var(--primary-color);
    }
    
    /* Tab Styling */
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
        margin-bottom: 20px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 55px;
        white-space: pre-wrap;
        background-color: white;
        border-radius: 8px;
        color: #666;
        font-weight: 600;
        border: 1px solid #e0e0e0;
        padding: 0 20px;
        transition: all 0.3s;
    }
    .stTabs [aria-selected="true"] {
        background-color: var(--secondary-color) !important;
        color: white !important;
        border: none;
        box-shadow: 0 4px 10px rgba(0,32,96,0.3);
    }
    
    /* Button Styling */
    div.stButton > button {
        background-color: var(--secondary-color);
        color: white;
        border: none;
        padding: 12px 24px;
        border-radius: 8px;
        font-weight: 700;
        transition: all 0.3s;
        width: 100%;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    div.stButton > button:hover {
        background-color: var(--primary-color);
        color: white;
        transform: translateY(-2px);
        box-shadow: 0 6px 12px rgba(235,0,139,0.3);
    }
    
    /* News Card Styling */
    .news-card {
        background-color: white;
        padding: 20px;
        border-radius: 12px;
        margin-bottom: 15px;
        border-left: 5px solid #ddd;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        transition: all 0.2s;
    }
    .news-card:hover {
        transform: translateX(5px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.1);
    }
    .news-card.source-naver { border-left-color: #03C75A; }
    .news-card.source-google { border-left-color: #4285F4; }
    .news-card.source-fss { border-left-color: #002060; }
    .news-card.source-fsc { border-left-color: #EB008B; }
    
    .news-badge {
        display: inline-block;
        padding: 4px 10px;
        border-radius: 20px;
        font-size: 0.75rem;
        font-weight: 700;
        color: white;
        margin-bottom: 8px;
        text-transform: uppercase;
    }
    .badge-naver { background-color: #03C75A; }
    .badge-google { background-color: #4285F4; }
    .badge-fss { background-color: #002060; }
    .badge-fsc { background-color: #EB008B; }
    
    h4 {
        margin: 5px 0 10px 0 !important;
        font-size: 1.1rem !important;
        font-weight: 700 !important;
        line-height: 1.4 !important;
    }
    
    /* Perplexity Report Styling */
    .perplexity-report-container {
        background-color: #ffffff;
        padding: 35px;
        border-radius: 15px;
        box-shadow: 0 10px 30px rgba(0,0,0,0.08);
        border: 1px solid #e0e0e0;
        margin-top: 25px;
        font-family: 'Noto Sans KR', sans-serif;
        color: #333;
        line-height: 1.7;
    }
    .perplexity-report-container h3 {
        color: #002060;
        border-bottom: 3px solid #EB008B;
        padding-bottom: 12px;
        margin-top: 30px;
        margin-bottom: 20px;
        font-size: 1.4rem;
        font-weight: 800;
        letter-spacing: -0.5px;
    }
    .perplexity-report-container h4 {
        background-color: #f8f9fa;
        padding: 15px 20px;
        border-left: 6px solid #002060;
        color: #333;
        margin-top: 25px;
        margin-bottom: 15px;
        border-radius: 0 8px 8px 0;
        font-size: 1.15rem;
        font-weight: 700;
        box-shadow: 0 2px 5px rgba(0,0,0,0.03);
    }
    .perplexity-report-container strong {
        color: #EB008B;
        font-weight: 700;
        background-color: rgba(235, 0, 139, 0.05);
        padding: 0 4px;
        border-radius: 4px;
    }
    .perplexity-report-container ul {
        margin-bottom: 20px;
        padding-left: 25px;
    }
    .perplexity-report-container li {
        margin-bottom: 8px;
    }
</style>
""", unsafe_allow_html=True)

# -----------------------------------------------------------------------------
# 2. Helper Classes (Logic)
# -----------------------------------------------------------------------------
class AuditDataGenerator:
    """Generates synthetic corporate card data with injected anomalies."""
    def __init__(self):
        self.fake = Faker('ko_KR')
        self.employees = [self.fake.name() for _ in range(50)]
        self.departments = ['IBì‚¬ì—…ë¶€', 'ë¦¬í…Œì¼ê¸ˆìœµíŒ€', 'ITê°œë°œíŒ€', 'ë¦¬ìŠ¤í¬ê´€ë¦¬íŒ€', 'ì¸ì‚¬íŒ€', 'ë²•ì¸ì˜ì—…íŒ€']
        # Assign home regions to employees (excluding Yeouido which is the office location)
        self.regions = ['ê°•ë‚¨êµ¬', 'ì„œì´ˆêµ¬', 'ì†¡íŒŒêµ¬', 'ë§ˆí¬êµ¬', 'ìš©ì‚°êµ¬', 'ì„±ë™êµ¬', 'ë¶„ë‹¹êµ¬', 'ì¼ì‚°']
        self.office_region = 'ì˜ë“±í¬êµ¬(ì—¬ì˜ë„)'
        self.employee_homes = {emp: random.choice(self.regions) for emp in self.employees}
        
    def generate_base_data(self, n_rows=10000):
        data = []
        start_date = datetime.now() - timedelta(days=90)
        
        for _ in range(n_rows):
            # Normal transaction logic
            dt = start_date + timedelta(days=random.randint(0, 90), 
                                      hours=random.randint(9, 22), 
                                      minutes=random.randint(0, 59))
            is_holiday = dt.weekday() >= 5 # 5=Sat, 6=Sun
            
            emp_name = random.choice(self.employees)
            
            # Normal transactions mostly happen near office or business districts
            if random.random() < 0.8:
                merchant_region = self.office_region
            else:
                merchant_region = random.choice(self.regions + ['ì¢…ë¡œêµ¬', 'ì¤‘êµ¬'])
                
            row = {
                'transaction_time': dt,
                'amount': round(random.lognormvariate(10, 1) * 1000, -2), # Log-normal distribution
                'merchant_name': self.fake.company() + " ì‹ë‹¹",
                'merchant_region': merchant_region,
                'mcc_code': 'ì¼ë°˜ìŒì‹ì ',
                'employee_name': 'ê¹€OO',
                'home_region': self.employee_homes[emp_name],
                'department': 'OOíŒ€',
                'is_holiday': is_holiday,
                'anomaly_type': 'Normal'
            }
            data.append(row)
        return pd.DataFrame(data)

    def inject_anomalies(self, df):
        anomalies = []
        
        # Scenario A: Split Payments (ìª¼ê°œê¸° ê²°ì œ)
        for _ in range(35):
            base_time = df['transaction_time'].sample().values[0]
            base_time = pd.to_datetime(base_time)
            emp = random.choice(self.employees)
            dept = random.choice(self.departments)
            merchant = "í•œìš° ì˜¤ë§ˆì¹´ì„¸ " + self.fake.word()
            region = self.office_region # Usually near office
            
            total_amount = random.randint(500000, 1500000)
            split_count = random.randint(2, 4)
            amount_per_txn = total_amount // split_count
            
            for i in range(split_count):
                anomalies.append({
                    'transaction_time': base_time + timedelta(minutes=i*2),
                    'amount': amount_per_txn,
                    'merchant_name': merchant,
                    'merchant_region': region,
                    'mcc_code': 'ì¼ë°˜ìŒì‹ì ',
                    'employee_name': 'ê¹€OO',
                    'home_region': self.employee_homes[emp],
                    'department': 'OOíŒ€',
                    'is_holiday': base_time.weekday() >= 5,
                    'anomaly_type': 'Split Payment'
                })

        # Scenario B: Late Night / Holiday / Entertainment (ì‹¬ì•¼/íœ´ì¼ ìœ í¥)
        for _ in range(20):
            base_time = df['transaction_time'].sample().values[0]
            base_time = pd.to_datetime(base_time).replace(hour=random.choice([23, 0, 1, 2, 3]))
            emp = random.choice(self.employees)
            
            anomalies.append({
                'transaction_time': base_time,
                'amount': random.randint(200000, 800000),
                'merchant_name': random.choice(['ê°•ë‚¨ ë£¸ì‹¸ë¡±', 'VIP ë…¸ë˜ë°©', 'í™©ì œ ìœ í¥ì£¼ì ']),
                'merchant_region': 'ê°•ë‚¨êµ¬', # Entertainment district
                'mcc_code': 'ìœ í¥ì£¼ì ',
                'employee_name': 'ê¹€OO',
                'home_region': self.employee_homes[emp],
                'department': 'OOíŒ€',
                'is_holiday': base_time.weekday() >= 5,
                'anomaly_type': 'Restricted Time/Sector'
            })

        # Scenario C: Clean Card Violation (Misleading Merchant Name)
        for _ in range(15):
            base_time = df['transaction_time'].sample().values[0]
            base_time = pd.to_datetime(base_time)
            emp = random.choice(self.employees)
            
            anomalies.append({
                'transaction_time': base_time,
                'amount': random.randint(150000, 400000),
                'merchant_name': 'ì‹œí¬ë¦¿ Bar ' + self.fake.word(),
                'merchant_region': random.choice(self.regions), # Random location
                'mcc_code': 'ì¼ë°˜ìŒì‹ì ', # Disguised as restaurant
                'employee_name': 'ê¹€OO',
                'home_region': self.employee_homes[emp],
                'department': 'OOíŒ€',
                'is_holiday': base_time.weekday() >= 5,
                'anomaly_type': 'Clean Card Violation'
            })

        # Scenario D: Personal Expense Near Home (ìíƒ ê·¼ì²˜ ê²°ì œ)
        for _ in range(25):
            base_time = df['transaction_time'].sample().values[0]
            base_time = pd.to_datetime(base_time)
            # Weekend or Late Night usually
            if random.random() > 0.5:
                base_time = base_time.replace(hour=random.choice([20, 21, 22, 10, 11])) # Late night or weekend brunch
            
            emp = random.choice(self.employees)
            home = self.employee_homes[emp]
            
            anomalies.append({
                'transaction_time': base_time,
                'amount': random.randint(50000, 200000), # Not necessarily huge amount
                'merchant_name': f"{home} {self.fake.word()} ë§ˆíŠ¸",
                'merchant_region': home, # Match Home Region
                'mcc_code': 'ë§ˆíŠ¸/í¸ì˜ì ',
                'employee_name': 'ê¹€OO',
                'home_region': home,
                'department': 'OOíŒ€',
                'is_holiday': True, # Often weekends
                'anomaly_type': 'Personal Expense (Near Home)'
            })
            
        return pd.concat([df, pd.DataFrame(anomalies)], ignore_index=True)

class AnomalyDetector:
    """Detects anomalies using XGBoost (Supervised Learning)."""
    def train_and_predict(self, df):
        # 1. Preprocessing
        # Create Target Variable (Labeling)
        df['target'] = df['anomaly_type'].apply(lambda x: 0 if x == 'Normal' else 1)
        
        # Feature Engineering
        df['hour'] = df['transaction_time'].dt.hour
        df['day_of_week'] = df['transaction_time'].dt.dayofweek
        df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)
        
        # New Feature: Is Near Home?
        df['is_near_home'] = (df['merchant_region'] == df['home_region']).astype(int)
        
        # Encode Categorical Variables
        le_dept = LabelEncoder()
        df['department_encoded'] = le_dept.fit_transform(df['department'])
        
        le_mcc = LabelEncoder()
        df['mcc_code_encoded'] = le_mcc.fit_transform(df['mcc_code'])
        
        # Select Features for Training
        features = ['amount', 'hour', 'is_weekend', 'department_encoded', 'mcc_code_encoded', 'is_near_home']
        X = df[features]
        y = df['target']
        
        # 2. Split Data
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
        
        # 3. Train XGBoost Model
        model = xgb.XGBClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            use_label_encoder=False,
            eval_metric='logloss',
            random_state=42
        )
        model.fit(X_train, y_train)
        
        # 4. Predict & Score
        # We use the probability of class 1 (Anomaly) as the score
        df['anomaly_score'] = model.predict_proba(X)[:, 1]
        
        # Thresholding (e.g., probability > 0.5 is an anomaly)
        # Since we have ground truth labels in this simulation, we can also just use the prediction
        df['is_anomaly_detected'] = model.predict(X) == 1
        
        # Optional: Print accuracy to console for debugging
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        print(f"XGBoost Model Accuracy: {acc:.4f}")
        
        return df

class NewsCrawler:
    """Crawls Naver News, Google News, FSS, and FSC."""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

    def get_all_news(self, pplx_api_key=None):
        """Aggregates news. Uses Perplexity if key provided, otherwise legacy crawlers."""
        all_news = []
        
        # 1. Official Sources (Always Crawl Direct URLs for accuracy)
        all_news.extend(self.crawl_fsc())
        all_news.extend(self.crawl_fss())
        
        # 2. Internet News (Naver & Google) - Broad Crawling
        # We crawl broadly and then let AI filter the results
        naver_queries = ["ì¦ê¶Œì‚¬ ê¸ˆìœµì‚¬ê³ ", "ê¸ˆìœµê°ë…ì› ì œì¬", "ì£¼ê°€ì¡°ì‘", "íš¡ë ¹ ë°°ì„", "ìë³¸ì‹œì¥ë²• ìœ„ë°˜"]
        for q in naver_queries:
            all_news.extend(self.crawl_naver(q))
            time.sleep(0.2)
        
        google_queries = ["ê¸ˆìœµê°ë…ì› ì œì¬", "ì¦ê¶Œì‚¬ ë‚´ë¶€í†µì œ", "ê¸ˆìœµì‚¬ê³ "]
        for q in google_queries:
            all_news.extend(self.crawl_google_rss(q))

        # 3. AI-Curated News (Perplexity)
        if pplx_api_key:
            # Use Perplexity for high-quality, summarized news search
            pplx_news = self.fetch_news_with_perplexity(pplx_api_key)
            if isinstance(pplx_news, list):
                all_news.extend(pplx_news)
            else:
                print("Perplexity Fallback triggered due to API error.")
        
        # Deduplicate by title
        unique_news = list({news['title']: news for news in all_news}.values())
        
        # Filter & Rank
        if pplx_api_key:
            # AI-Based Filtering (Verification)
            filtered_news = self.filter_news_with_ai(unique_news, pplx_api_key)
        else:
            # Rule-Based Filtering
            filtered_news = [n for n in unique_news if self.is_relevant(n)]
        
        # Rank
        ranked_news = self.rank_news(filtered_news)
        
        return ranked_news

    def rank_news(self, news_list):
        """Rank news by importance using weighted keywords."""
        # High Risk Keywords (Weight: 3)
        critical_keywords = ['íš¡ë ¹', 'ë°°ì„', 'êµ¬ì†', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì œì¬', 'ê³¼ì§•ê¸ˆ', 'ì˜ì—…ì •ì§€', 'ë“±ë¡ì·¨ì†Œ', 'ê²€ì°°', 'ê³ ë°œ']
        # Medium Risk Keywords (Weight: 2)
        warning_keywords = ['ì£¼ì˜', 'ê²½ê³ ', 'ì ë°œ', 'ìœ„ë°˜', 'ë¶ˆê³µì •', 'ì¡°ì‘', 'ë¯¸ê³µê°œ', 'ì†ì‹¤', 'ë¶€ì‹¤', 'ì‚¬ê³ ', 'ê²€ì‚¬']
        # Low Risk Keywords (Weight: 1)
        general_keywords = ['ê¸ˆìœµìœ„', 'ê¸ˆê°ì›', 'ê°ë…', 'ê·œì œ', 'ê°œì •', 'ë°œí‘œ']
        
        scored_news = []
        for news in news_list:
            score = 0
            title = news['title']
            summary = news['summary']
            combined = title + " " + summary
            
            for k in critical_keywords:
                if k in combined: score += 3
            for k in warning_keywords:
                if k in combined: score += 2
            for k in general_keywords:
                if k in combined: score += 1
                
            score += random.random() # Tie-breaker
            news['score'] = score
            scored_news.append(news)
            
        return sorted(scored_news, key=lambda x: x['score'], reverse=True)

    def is_relevant(self, news):
        """Checks if the news is relevant using detailed audit keywords."""
        # 1. Critical Risk (Must Catch)
        risk_keywords = [
            'íš¡ë ¹', 'ë°°ì„', 'ì°¨ëª…', 'ì„ í–‰ë§¤ë§¤', 'ìŠ¤ìº˜í•‘', 'ì¼ì„ë§¤ë§¤', 'ê³¼ë‹¹ë§¤ë§¤', 'ë¶€ë‹¹ê¶Œìœ ', 'ì´ë©´ê³„ì•½', 'êº¾ê¸°', 'ìê¸ˆì„¸íƒ', 'ë¦¬ë² ì´íŠ¸', # Fraud
            'ì‹œì„¸ì¡°ì¢…', 'ì£¼ê°€ì¡°ì‘', 'í†µì •ë§¤ë§¤', 'ê°€ì¥ë§¤ë§¤', 'ë¯¸ê³µê°œì •ë³´', 'ë¬´ì°¨ì…ê³µë§¤ë„', 'í—ˆìˆ˜ì£¼ë¬¸', 'ë¸”ë¡ë”œ', 'ìì „ê±°ë˜', 'ì±„ê¶ŒíŒŒí‚¹', 'ìœˆë„ìš°ë“œë ˆì‹±', # Market Manipulation
            'PFë¶€ì‹¤', 'ê¸°í•œì´ìµìƒì‹¤', 'EOD', 'ë¸Œë¦¿ì§€ë¡ ', 'ì±…ì„ì¤€ê³µ', 'ìš°ë°œì±„ë¬´', 'ëŒ€ì†ì¶©ë‹¹ê¸ˆ', 'ìˆœìë³¸ë¹„ìœ¨', 'NCR', 'ìœ ë™ì„±ë¹„ìœ¨', 'LCR', # IB/Risk
            'ë¶ˆì™„ì „íŒë§¤', 'ELS', 'DLS', 'ë©ì–´ì¹´ìš´íŠ¸', 'ì‚¬ëª¨í€ë“œ', 'í™˜ë§¤ì¤‘ë‹¨', 'ì›ê¸ˆì†ì‹¤', 'í•´í”¼ì½œ', # Consumer Protection
            'ì „ì‚°ì¥ì• ', 'ë§ë¶„ë¦¬', 'ê°œì¸ì •ë³´ìœ ì¶œ', 'ì ‘ê·¼í†µì œ', 'DDoS', 'ëœì„¬ì›¨ì–´', 'ì´ìƒê¸ˆìœµê±°ë˜', 'FDS', 'ì˜¤í”ˆAPI', 'í´ë¼ìš°ë“œ', # IT
            'ì±…ë¬´êµ¬ì¡°ë„', 'ë‚´ë¶€í†µì œ', 'ê¸°ê´€ê²½ê³ ', 'ê¸°ê´€ì£¼ì˜', 'ì„ì›ë¬¸ì±…', 'ì§ë¬´ì •ì§€', 'ê³¼ì§•ê¸ˆ', 'ê³¼íƒœë£Œ', 'ê³µì‹œìœ„ë°˜', 'ëŒ€ì£¼ì£¼ì ê²©ì„±', # Regulation
            'ë¶„ì‹íšŒê³„', 'ë²•ì¸ì¹´ë“œ', 'ì ‘ëŒ€ë¹„', 'ê°€ì§€ê¸‰ê¸ˆ', 'ì„±í¬ë¡±', 'ì±„ìš©ë¹„ë¦¬', 'ë‚´ë¶€ê³ ë°œ', 'STO', 'ê°€ìƒìì‚°' # General/New Biz
        ]
        
        # 2. Irrelevant Contexts (Filter Out)
        ignore_keywords = ['ì±„ìš©ê³µê³ ', 'ì´ë²¤íŠ¸', 'ìš°ìŠ¹', 'ìŠ¤í¬ì¸ ', 'ë‚ ì”¨', 'ë¶€ê³ ', 'ì¸ì‚¬ë™ì •', 'ê´‘ê³ ', 'í™ë³´', 'ìº í˜ì¸', 'ë´‰ì‚¬í™œë™', 'MOUì²´ê²°']
        
        title = news['title']
        summary = news['summary']
        combined = (title + " " + summary)
        
        # Filter out irrelevant
        if any(bad in combined for bad in ignore_keywords):
            return False
            
        # Check for relevant keywords
        if any(good in combined for good in risk_keywords):
            return True
            
        return False

    def filter_news_with_ai(self, news_list, api_key):
        """Uses Perplexity to verify relevance of news items."""
        if not news_list: return []
        
        # Prepare list for prompt
        news_text = "\n".join([f"{i}. {n['title']} (Summary: {n['summary']})" for i, n in enumerate(news_list)])
        
        system_prompt = """
        You are an expert Audit Assistant for Kiwoom Securities.
        Review the provided list of news items. 
        Select ONLY the items that are critical for the Internal Audit Team (Fraud, Regulation, Risk, IT Security, Consumer Protection).
        Discard marketing, general market news, or irrelevant items.
        Return ONLY a JSON array of the INDICES (0-based integers) of the relevant items.
        Example: [0, 2, 5]
        """
        
        payload = {
            "model": "sonar",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"News List:\n{news_text}"}
            ]
        }
        
        headers = {
            "Authorization": f"Bearer {api_key.strip()}",
            "Content-Type": "application/json"
        }
        
        try:
            response = requests.post("https://api.perplexity.ai/chat/completions", json=payload, headers=headers)
            if response.status_code == 200:
                content = response.json()['choices'][0]['message']['content']
                # Extract JSON array
                match = re.search(r'\[.*\]', content, re.DOTALL)
                if match:
                    indices = json.loads(match.group(0))
                    return [news_list[i] for i in indices if i < len(news_list)]
        except Exception as e:
            print(f"AI Filter Error: {e}")
            
        # Fallback to rule-based if AI fails
        return [n for n in news_list if self.is_relevant(n)]

    @staticmethod
    def fetch_news_with_perplexity(api_key):
        """Fetches latest financial news using Perplexity API."""
        url = "https://api.perplexity.ai/chat/completions"
        
        # Specific Prompt for Korean Securities Risks
        # Specific Prompt for Korean Securities Risks (Updated with detailed categories)
        system_prompt = """
        You are a specialized news aggregator for 'Kiwoom Securities' Audit Team. 
        Focus ONLY on South Korean financial news related to the following critical risk categories:
        
        1. Fraud & Embezzlement: Embezzlement, Breach of Trust, Borrowed Name Accounts, Front Running, Scalping, Churning, Rebates.
        2. Market Manipulation: Stock Manipulation, Insider Trading, Naked Short Selling, High Frequency Trading (HFT) Risks, Bond Parking, Window Dressing.
        3. IB & Credit Risk: PF Default, EOD, Bridge Loan Risks, Contingent Liabilities, NCR/LCR Issues.
        4. Consumer Protection: Misselling, ELS/DLS Knock-in, Fund Redemption Suspension.
        5. IT Security: System Failure, Network Separation Violation, Data Leakage, DDoS, Cloud Risks.
        6. Regulation: FSS/FSC Sanctions, CEO Risks, Disclosure Violations, Governance Issues.
        
        Exclude general market analysis, stock price updates, ESG campaigns, or global economy news unless directly relevant to these compliance risks.
        """
        user_prompt = "Search for the most recent (last 7 days) critical news items fitting the criteria. Provide 15 distinct items. Return ONLY a JSON array with keys: 'title', 'summary', 'source', 'link'. The content must be in Korean."
        
        payload = {
            "model": "sonar",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        }
        
        headers = {
            "Authorization": f"Bearer {api_key.strip()}",
            "Content-Type": "application/json",
            "User-Agent": "KiwoomAuditSystem/2.2"
        }
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            if response.status_code == 200:
                content = response.json()['choices'][0]['message']['content']
                content = content.replace("```json", "").replace("```", "").strip()
                try:
                    news_data = json.loads(content)
                    return news_data
                except json.JSONDecodeError:
                    return f"âš ï¸ Failed to parse Perplexity response: {content[:100]}..."
            else:
                return f"âš ï¸ Perplexity API Error: {response.status_code} - {response.text}"
        except Exception as e:
            return f"âš ï¸ API Call Failed: {str(e)}"

    def crawl_naver(self, query):
        url = f"https://search.naver.com/search.naver?where=news&query={query}&sm=tab_opt&sort=1&photo=0&field=0&pd=0&ds=&de=&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Add%2Cp%3Aall&is_sug_officeid=0"
        
        news_list = []
        try:
            response = requests.get(url, headers=self.headers, timeout=5)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                items = soup.select('.news_area')
                
                for item in items[:3]:
                    title_tag = item.select_one('.news_tit')
                    if title_tag:
                        title = title_tag.get_text()
                        link = title_tag['href']
                        dsc = item.select_one('.news_dsc')
                        summary = dsc.get_text() if dsc else "ìš”ì•½ ì—†ìŒ"
                        news_list.append({'title': title, 'link': link, 'summary': summary, 'source': 'Naver'})
        except Exception as e:
            print(f"Naver Crawl Error: {e}")
        return news_list

    def crawl_google_rss(self, query):
        """Crawls Google News via RSS."""
        url = f"https://news.google.com/rss/search?q={query}&hl=ko&gl=KR&ceid=KR:ko"
        news_list = []
        try:
            response = requests.get(url, timeout=5)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, features='xml')
                items = soup.find_all('item')
                
                for item in items[:3]:
                    title = item.title.text
                    link = item.link.text
                    desc_html = item.description.text
                    summary = BeautifulSoup(desc_html, 'html.parser').get_text()[:100] + "..."
                    news_list.append({'title': title, 'link': link, 'summary': summary, 'source': 'Google'})
        except Exception as e:
            print(f"Google RSS Error: {e}")
        return news_list

    def crawl_fss(self):
        """Financial Supervisory Service - Improved Crawling with Fallback Labeling"""
        target_url = "https://www.fss.or.kr/fss/bbs/B0000188/list.do?menuNo=200218"
        news_list = []
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            rows = soup.select('.bd-list tbody tr')
            
            for row in rows[:5]:
                title_tag = row.select_one('.subject a')
                if title_tag:
                    title = title_tag.get_text().strip()
                    href = title_tag['href']
                    date_tag = row.select_one('.date')
                    date = date_tag.get_text().strip() if date_tag else ""
                    full_link = "https://www.fss.or.kr" + href if href.startswith('/') else href
                    news_list.append({
                        'title': f"[ê¸ˆê°ì›] {title}",
                        'link': full_link,
                        'summary': f"ë“±ë¡ì¼: {date} | ê¸ˆìœµê°ë…ì› ë³´ë„ìë£Œì…ë‹ˆë‹¤.",
                        'source': 'FSS'
                    })
            
            if not news_list:
                # Fallback to Google News but label as FSS
                fallback = self.crawl_google_rss("ê¸ˆìœµê°ë…ì› ë³´ë„ìë£Œ")
                for item in fallback:
                    item['source'] = 'FSS (via Google)'
                return fallback
                
        except Exception as e:
            print(f"FSS Crawl Error: {e}")
            fallback = self.crawl_google_rss("ê¸ˆìœµê°ë…ì›")
            for item in fallback:
                item['source'] = 'FSS (via Google)'
            return fallback
            
        return news_list

    def crawl_fsc(self):
        """Financial Services Commission - Improved Crawling with Fallback Labeling"""
        target_url = "https://www.fsc.go.kr/no010101"
        news_list = []
        try:
            response = requests.get(target_url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # More robust finding: Look for links containing the board path
            links = soup.find_all('a', href=True)
            
            for a in links:
                href = a['href']
                # Filter for specific article links (usually contain /no010101/ + a number)
                if '/no010101/' in href and any(c.isdigit() for c in href):
                    title = a.get_text().strip()
                    if len(title) < 10: continue # Skip short nav links
                    
                    # Deduplicate
                    if any(n['title'] == f"[ê¸ˆìœµìœ„] {title}" for n in news_list):
                        continue
                        
                    full_link = "https://www.fsc.go.kr" + href if href.startswith('/') else href
                    
                    news_list.append({
                        'title': f"[ê¸ˆìœµìœ„] {title}",
                        'link': full_link,
                        'summary': "ê¸ˆìœµìœ„ì›íšŒ ë³´ë„ìë£Œ ë° ì£¼ìš” ì •ì±… ë°œí‘œì…ë‹ˆë‹¤.",
                        'source': 'FSC'
                    })
                    if len(news_list) >= 5: break
                
            if not news_list:
                 # Fallback to Google News but label as FSC
                fallback = self.crawl_google_rss("ê¸ˆìœµìœ„ì›íšŒ ë³´ë„ìë£Œ")
                for item in fallback:
                    item['source'] = 'FSC (via Google)'
                return fallback

        except Exception as e:
            print(f"FSC Crawl Error: {e}")
            fallback = self.crawl_google_rss("ê¸ˆìœµìœ„ì›íšŒ")
            for item in fallback:
                item['source'] = 'FSC (via Google)'
            return fallback
            
        return news_list



class GenAISimulator:
    """Simulates LLM text generation."""
    @staticmethod
    def generate_audit_report(anomaly_row):
        merchant = anomaly_row['merchant_name']
        amount = f"{anomaly_row['amount']:,}"
        time_str = anomaly_row['transaction_time'].strftime('%Y-%m-%d %H:%M')
        emp = anomaly_row['employee_name']
        
        prompt_context = ""
        if "Split" in str(anomaly_row.get('anomaly_type', '')):
            prompt_context = "ë™ì¼ ê°€ë§¹ì  ë‹¨ì‹œê°„ ë°˜ë³µ ê²°ì œ(ìª¼ê°œê¸° ê²°ì œ) ì˜ì‹¬"
        elif "Restricted" in str(anomaly_row.get('anomaly_type', '')):
            prompt_context = "ì‹¬ì•¼/íœ´ì¼ ì œí•œ ì—…ì¢…(ìœ í¥) ê²°ì œ ì˜ì‹¬"
        elif "Clean" in str(anomaly_row.get('anomaly_type', '')):
            prompt_context = "í´ë¦°ì¹´ë“œ ê¸ˆì§€ ì—…ì¢… ìœ„ì¥ ê²°ì œ ì˜ì‹¬"
        elif "Personal" in str(anomaly_row.get('anomaly_type', '')):
            prompt_context = "ìíƒ ì¸ê·¼ ì‚¬ì  ìœ ìš© ì˜ì‹¬ (Personal Expense Near Home)"
        else:
            prompt_context = "í†µìƒì ì´ì§€ ì•Šì€ ê³ ì•¡ ê²°ì œ íŒ¨í„´"

        report_template = f"""
### ğŸ“‘ AI ê°ì‚¬ ì¡°ì„œ ì´ˆì•ˆ (Draft Audit Report)
**ìƒì„± ì¼ì‹œ:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ëŒ€ìƒ ì§ì›:** {emp} ({anomaly_row['department']})
---
#### 1. ìœ„ë°˜ í˜ì˜ ë¶„ì„ (Anomaly Analysis)
*   **íƒì§€ ìœ í˜•:** {prompt_context}
*   **ìƒì„¸ ë‚´ìš©:** {time_str}ì— '{merchant}'ì—ì„œ {amount}ì›ì´ ê²°ì œë˜ì—ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ê±´ì€ ë‚´ë¶€ í†µì œ ê¸°ì¤€(Rule-Set) ë° AI ì´ìƒ íƒì§€ ëª¨ë¸ì— ì˜í•´ **Risk Score 98ì **ìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤.
*   **íŠ¹ì´ ì‚¬í•­:** ë™ì‹œê°„ëŒ€ ìœ ì‚¬ ì—…ì¢… í‰ê·  ê²°ì œì•¡ ëŒ€ë¹„ 300% ì´ìƒ ë†’ìœ¼ë©°, ê²°ì œ íŒ¨í„´ì´ ë¹„ì •ìƒì ì…ë‹ˆë‹¤.

#### 2. ê´€ë ¨ ë‚´ë¶€ ê·œì • (Regulation Check)
*   **ì œ 3ì¡° 2í•­ (ë²•ì¸ì¹´ë“œ ì‚¬ìš© ì œí•œ):** ìœ í¥ì—…ì¢…, ê³¨í”„ì¥, ì‹¬ì•¼ ì‹œê°„ëŒ€(23:00~06:00) ì‚¬ìš©ì„ ì›ì¹™ì ìœ¼ë¡œ ê¸ˆì§€í•¨.
*   **ì œ 5ì¡° 1í•­ (ë¶„í•  ê²°ì œ ê¸ˆì§€):** ì „ê²° ê·œì • íšŒí”¼ë¥¼ ëª©ì ìœ¼ë¡œ í•œ ë¶„í•  ê²°ì œ(ì¼ëª… ìª¼ê°œê¸°)ëŠ” ì§•ê³„ ëŒ€ìƒì„.

#### 3. ì†Œëª… ìš”ì²­ ë° ì¡°ì¹˜ ê³„íš (Action Plan)
1.  **ì†Œëª… ìë£Œ ì œì¶œ ìš”êµ¬:** {emp} ì§ì›ì—ê²Œ í•´ë‹¹ ê²°ì œ ê±´ì— ëŒ€í•œ ì˜ìˆ˜ì¦ ë° ì‚¬ìœ ì„œ ì œì¶œ ìš”ì²­ (ê¸°í•œ: 3ì¼ ë‚´).
2.  **ë¶€ì„œì¥ í†µë³´:** {anomaly_row['department']}ì¥ì—ê²Œ ìœ„ë°˜ ì˜ì‹¬ ì‚¬ë¡€ í†µë³´ ë° ê´€ë¦¬ ê°ë… ê°•í™” ìš”ì²­.
3.  **í™˜ìˆ˜ ì¡°ì¹˜ ê²€í† :** ì†Œëª… ë¶ˆì¶©ë¶„ ì‹œ ì „ì•¡ í™˜ìˆ˜ ë° ì¸ì‚¬ìœ„ì›íšŒ íšŒë¶€ ê²€í† .
---
*ë³¸ ë³´ê³ ì„œëŠ” ìƒì„±í˜• AIê°€ ì‘ì„±í•œ ì´ˆì•ˆì´ë©°, ê°ì‚¬ì¸ì˜ ìµœì¢… ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.*
"""
        return report_template

    @staticmethod
    def generate_threat_analysis(news_title):
        # Determine context based on keywords in the title
        title_lower = news_title.lower()
        
        if any(k in title_lower for k in ['íš¡ë ¹', 'ë°°ì„', 'ìœ ìš©']):
            category = "ìê¸ˆ íš¡ë ¹ ë° ìœ ìš©"
            risk_analysis = "í•´ë‹¹ ê¸°ì‚¬ëŠ” ì„ì§ì›ì— ì˜í•œ ìê¸ˆ íš¡ë ¹ ë° ì‚¬ì  ìœ ìš© ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. íŠ¹íˆ ìê¸ˆ ì§‘í–‰ ê¶Œí•œì´ ì§‘ì¤‘ëœ ë¶€ì„œì—ì„œì˜ ë‚´ë¶€í†µì œ ì‹¤íŒ¨ê°€ ì£¼ìš” ì›ì¸ìœ¼ë¡œ ë¶„ì„ë©ë‹ˆë‹¤."
            checklist = """
*   **[ê¸´ê¸‰]** ìê¸ˆ ì§‘í–‰ ë¶€ì„œ(PF, ë²•ì¸ì˜ì—…)ì˜ **ì§ë¬´ ë¶„ë¦¬(Segregation of Duties)** ë° ìˆœí™˜ ê·¼ë¬´ í˜„í™© ì ê²€.
*   **[ìƒì‹œ]** ë²•ì¸ì¹´ë“œ ë° ê³„ì¢Œ ì´ì²´ ë‚´ì—­ì— ëŒ€í•œ ì´ìƒ ì§•í›„ ëª¨ë‹ˆí„°ë§ ê°•í™”.
*   **[ì‹œìŠ¤í…œ]** ê³ ì•¡ ìê¸ˆ ì´ì²´ ì‹œ ë‹¤ë‹¨ê³„ ìŠ¹ì¸ ì ˆì°¨(Multi-Approval) ìš°íšŒ ì—¬ë¶€ ì „ìˆ˜ ì¡°ì‚¬."""
            
        elif any(k in title_lower for k in ['ì œì¬', 'ê³¼ì§•ê¸ˆ', 'ê¸°ê´€ê²½ê³ ', 'ì¡°ì¹˜']):
            category = "ê¸ˆìœµë‹¹êµ­ ì œì¬ ë° ê·œì œ ìœ„ë°˜"
            risk_analysis = "ê¸ˆìœµê°ë…ì› ë° ê¸ˆìœµìœ„ì›íšŒì˜ ì œì¬ ì¡°ì¹˜ëŠ” íšŒì‚¬ì˜ í‰íŒ ë¦¬ìŠ¤í¬ì™€ ì§ê²°ë©ë‹ˆë‹¤. í•´ë‹¹ ê±´ì€ ë¶ˆì™„ì „ íŒë§¤ ë˜ëŠ” ê³µì‹œ ì˜ë¬´ ìœ„ë°˜ê³¼ ê´€ë ¨ëœ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤."
            checklist = """
*   **[ê¸´ê¸‰]** ìµœê·¼ 3ë…„ê°„ ìœ ì‚¬ ì‚¬ë¡€ì— ëŒ€í•œ ë‚´ë¶€ ê°ì‚¬ ê¸°ë¡ ì¬ê²€í† .
*   **[êµìœ¡]** ì „ ì„ì§ì› ëŒ€ìƒ ì»´í”Œë¼ì´ì–¸ìŠ¤(Compliance) ì¤€ìˆ˜ êµìœ¡ ê°•í™”.
*   **[ë³´ê³ ]** ì œì¬ ì›ì¸ ë¶„ì„ ë³´ê³ ì„œ ì‘ì„± ë° ì¬ë°œ ë°©ì§€ ëŒ€ì±… ì´ì‚¬íšŒ ë³´ê³ ."""
            
        elif any(k in title_lower for k in ['ì£¼ê°€', 'ì‹œì„¸', 'ë¶ˆê³µì •', 'ë¯¸ê³µê°œ']):
            category = "ë¶ˆê³µì • ê±°ë˜ ë° ì‹œì„¸ ì¡°ì¢…"
            risk_analysis = "ë¯¸ê³µê°œ ì •ë³´ ì´ìš© ë˜ëŠ” ì‹œì„¸ ì¡°ì¢… í˜ì˜ëŠ” ìë³¸ì‹œì¥ë²• ìœ„ë°˜ì˜ ì¤‘ëŒ€ ì‚¬ì•ˆì…ë‹ˆë‹¤. ì„ì§ì›ì˜ ìê¸°ë§¤ë§¤ ë° ì°¨ëª… ê³„ì¢Œ ìš´ìš© ê°€ëŠ¥ì„±ì„ ë°°ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            checklist = """
*   **[ê¸´ê¸‰]** ì„ì§ì› ìê¸°ë§¤ë§¤ ì‹ ê³  ë‚´ì—­ê³¼ ì‹¤ì œ ê±°ë˜ ë‚´ì—­ ëŒ€ì‚¬(Cross-Check).
*   **[ëª¨ë‹ˆí„°ë§]** ì‚¬ë‚´ ë©”ì‹ ì € ë° ì´ë©”ì¼ í‚¤ì›Œë“œ ê²€ìƒ‰ì„ í†µí•œ ë¯¸ê³µê°œ ì •ë³´ ìœ í†µ ì •í™© í¬ì°©.
*   **[ì‹œìŠ¤í…œ]** ë§¤ë§¤ ì£¼ë¬¸ ê¸°ë¡(Log) ë³´ì¡´ ìƒíƒœ ì ê²€."""
            
        else:
            category = "ê¸°íƒ€ ê¸ˆìœµ ì‚¬ê³  ë° ë¦¬ìŠ¤í¬"
            risk_analysis = "í•´ë‹¹ ê¸°ì‚¬ëŠ” ì¼ë°˜ì ì¸ ê¸ˆìœµê¶Œ ë¦¬ìŠ¤í¬ ë˜ëŠ” ì •ì±… ë³€í™”ë¥¼ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤. ì„ ì œì ì¸ ë‚´ë¶€ ê·œì • ì •ë¹„ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            checklist = """
*   **[ìƒì‹œ]** ê´€ë ¨ ë‚´ë¶€ ê·œì •(ì‚¬ê·œ)ì˜ í˜„í–‰ ë²•ê·œ ë¶€í•© ì—¬ë¶€ ê²€í† .
*   **[ëª¨ë‹ˆí„°ë§]** íƒ€ì‚¬ ì‚¬ë¡€ë¥¼ ë²¤ì¹˜ë§ˆí‚¹í•˜ì—¬ ë‚´ë¶€í†µì œ ì‚¬ê°ì§€ëŒ€ ë°œêµ´.
*   **[ì ê²€]** ë¦¬ìŠ¤í¬ ê´€ë¦¬ ìœ„ì›íšŒ ì•ˆê±´ ìƒì • ê²€í† ."""

        return f"""
### ğŸ›¡ï¸ ìƒì„±í˜• AI ìœ„í˜‘ ë¶„ì„ (Threat Intelligence)
**ë¶„ì„ ëŒ€ìƒ ë‰´ìŠ¤:** {news_title}
**ë¶„ë¥˜:** {category}
---
#### 1. ì‚¬ê±´ ê°œìš” ë° í•µì‹¬ ìœ„í—˜ (Key Risks)
{risk_analysis}

#### 2. í‚¤ì›€ì¦ê¶Œ ë‚´ë¶€ ì ê²€ í•„ìš” í•­ëª© (Internal Checklist)
{checklist}

#### 3. ëŒ€ì‘ ê°ì‚¬ ê³„íš (Audit Plan)
*   **ê°ì‚¬ ëª…:** {category} ëŒ€ì‘ íŠ¹ë³„/ìƒì‹œ ê°ì‚¬
*   **ì˜ˆìƒ ì†Œìš” ê¸°ê°„:** 2ì£¼ (AI ì‚¬ì „ ë¶„ì„ 3ì¼ + í˜„ì¥ ê°ì‚¬ 7ì¼)
---
*AI Analysis Completed.*
"""

    @staticmethod
    def generate_analysis_with_perplexity(api_key, news_item):
        """Calls Perplexity API for deep analysis using RAG (Retrieval-Augmented Generation)."""
        # Basic Validation
        if not api_key.startswith("pplx-"):
            return "âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ API Key í˜•ì‹ì…ë‹ˆë‹¤. 'pplx-'ë¡œ ì‹œì‘í•˜ëŠ” í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."

        news_title = news_item.get('title', 'ì œëª© ì—†ìŒ')
        news_link = news_item.get('link', '')
        
        # 1. RAG: Extract Full Content using newspaper3k
        full_text = ""
        try:
            if news_link and news_link != "#":
                article = Article(news_link, language='ko')
                article.download()
                article.parse()
                full_text = article.text[:3000] # Limit context window if necessary
        except Exception as e:
            full_text = f"(ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)})"

        # 2. Construct Prompt with Full Context
        system_prompt = """
        ë‹¹ì‹ ì€ í‚¤ì›€ì¦ê¶Œ ë‚´ë¶€ê°ì‚¬íŒ€ì˜ ìˆ˜ì„ ê°ì‚¬ì—­(Chief Auditor) AIì…ë‹ˆë‹¤.
        ì œê³µëœ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ **ë³¸ë¬¸(Full Text)**ì„ ì •ë°€ ë¶„ì„í•˜ì—¬, ìš°ë¦¬ íšŒì‚¬(ì¦ê¶Œì‚¬)ì— ë¯¸ì¹  ìˆ˜ ìˆëŠ” ì ì¬ì  ìœ„í˜‘ì„ ì‹ë³„í•˜ê³  êµ¬ì²´ì ì¸ ê°ì‚¬ ëŒ€ì‘ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìˆ˜ë¦½í•˜ì„¸ìš”.
        
        ë°˜ë“œì‹œ ë‹¤ìŒ 4ê°€ì§€ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±ëœ ì „ë¬¸ì ì¸ ê°ì‚¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”:
        1. ğŸ” **ì‚¬ê±´ ì‹¬ì¸µ ìš”ì•½ (Executive Summary)**: ê¸°ì‚¬ì˜ í•µì‹¬ íŒ©íŠ¸ì™€ ì—°ë£¨ëœ ê¸ˆìœµ ì‚¬ê³  ìœ í˜•ì„ ëª…í™•íˆ ìš”ì•½.
        2. âš ï¸ **í•µì‹¬ ë¦¬ìŠ¤í¬ ì‹ë³„ (Key Risk Indicators)**: ì´ ì‚¬ê±´ì´ ìš°ë¦¬ íšŒì‚¬ì—ì„œ ë°œìƒí•  ê²½ìš° ì˜ˆìƒë˜ëŠ” ë²•ì , ì¬ë¬´ì , í‰íŒ ë¦¬ìŠ¤í¬.
        3. ğŸ›¡ï¸ **ê°ì‚¬ ëŒ€ì‘ ì‹œë‚˜ë¦¬ì˜¤ (Audit Response Scenario)**: 
           - ë§Œì•½ ì´ ì‚¬ê±´ì´ ìš°ë¦¬ íšŒì‚¬ì—ì„œ ë°œìƒí–ˆë‹¤ë©´, ì–´ë–¤ ë°ì´í„°ì™€ ë¡œê·¸ë¥¼ í™•ì¸í•´ì•¼ í•˜ëŠ”ê°€?
           - êµ¬ì²´ì ì¸ ê°ì‚¬ ì ˆì°¨(Audit Procedure)ì™€ ì ë°œ ê¸°ë²•.
        4. âœ… **ì¦‰ì‹œ ì ê²€ ì²´í¬ë¦¬ìŠ¤íŠ¸ (Actionable Checklist)**: ë‚´ì¼ ë‹¹ì¥ í˜„ì—… ë¶€ì„œì— ë°°í¬í•  êµ¬ì²´ì ì¸ ì ê²€ í•­ëª© (ë¶€ì„œëª… ëª…ì‹œ).
        
        ë‹µë³€ì€ í‚¤ì›€ì¦ê¶Œì˜ ë‚´ë¶€ ë³´ê³ ì„œ í†¤ì•¤ë§¤ë„ˆ(ì „ë¬¸ì , ì§ê´€ì , í•µì‹¬ ìœ„ì£¼)ë¥¼ ìœ ì§€í•˜ì„¸ìš”.
        """
        
        user_prompt = f"""
        [ë¶„ì„ ëŒ€ìƒ ë‰´ìŠ¤]
        - ì œëª©: {news_title}
        - ë§í¬: {news_link}
        - ë³¸ë¬¸ ë‚´ìš©:
        {full_text}
        
        ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì¸µ ê°ì‚¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.
        """

        url = "https://api.perplexity.ai/chat/completions"
        
        payload = {
            "model": "sonar", 
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        }
        
        headers = {
            "Authorization": f"Bearer {api_key.strip()}",
            "Content-Type": "application/json",
            "User-Agent": "KiwoomAuditSystem/2.3"
        }
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            
            try:
                result = response.json()
            except ValueError:
                return f"âš ï¸ API Error (Non-JSON Response): {response.status_code} - {response.text[:200]}..."

            if response.status_code == 200:
                content = result['choices'][0]['message']['content']
                
                # Post-process Markdown for Kiwoom Styling
                content = re.sub(r'### (.*)', r'<h3>\1</h3>', content)
                content = re.sub(r'#### (.*)', r'<h4>\1</h4>', content)
                content = re.sub(r'\*\*(.*?)\*\*', r'<strong style="color: #EB008B;">\1</strong>', content)
                
                return content
            else:
                error_msg = result.get('error', {}).get('message', 'Unknown Error')
                return f"âš ï¸ Perplexity API Error: {response.status_code} - {error_msg}"
        except Exception as e:
            return f"âš ï¸ API Call Failed: {str(e)}"

# -----------------------------------------------------------------------------
# 3. Main Application Logic
# -----------------------------------------------------------------------------
def main():
    # Sidebar
    with st.sidebar:
        st.image("logo.png", width=200)
        st.markdown("---")
        st.header("âš™ï¸ ì‹œìŠ¤í…œ ì œì–´")
        
        audit_date = st.date_input("ê°ì‚¬ ê¸°ì¤€ì¼", datetime.now())
        
        st.markdown("---")
        st.markdown("### ğŸ”‘ API Key ì„¤ì •")
        pplx_api_key = st.text_input("Perplexity API Key", type="password", help="ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ìš© (Perplexity Pro)")
        
        st.info(f"ê¸°ì¤€ì¼: {audit_date.strftime('%Y-%m-%d')}")
        
        st.markdown("---")
        st.caption("Developed for Kiwoom Securities Audit Team")
        st.caption("v2.3.0 (All-in-One Perplexity)")

    # Tabs
    tab1, tab2 = st.tabs(["ğŸ“Š ë‚´ë¶€ ë°ì´í„° ê°ì‚¬ (Internal Audit)", "ğŸŒ ì™¸ë¶€ ìœ„í˜‘ ëŒ€ì‘ (Threat Intelligence)"])
    
    # -------------------------------------------------------------------------
    # TAB 1: Internal Audit Simulation
    # -------------------------------------------------------------------------
    with tab1:
        st.subheader("ğŸ’³ ë²•ì¸ì¹´ë“œ ì´ìƒ ì§•í›„ íƒì§€ (FDS)")
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            st.markdown("#### 1. ë°ì´í„° ìƒì„± ë° ë¶„ì„")
            st.write("ìµœê·¼ 3ê°œì›”ì¹˜ ë²•ì¸ì¹´ë“œ ì‚¬ìš© ë‚´ì—­ 10,000ê±´ì„ ìƒì„±í•˜ê³ , XGBoost (Supervised Learning) ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì´ìƒ ì§•í›„ë¥¼ íƒì§€í•©ë‹ˆë‹¤.")
            
            if st.button("ğŸš€ ë°ì´í„° ìƒì„± ë° AI ë¶„ì„ ì‹œì‘", key="btn_run_audit"):
                with st.spinner("ë°ì´í„° ìƒì„± ë° ì´ìƒ íƒì§€ ëª¨ë¸ êµ¬ë™ ì¤‘..."):
                    # 1. Generate Data
                    generator = AuditDataGenerator()
                    df = generator.generate_base_data()
                    df = generator.inject_anomalies(df)
                    
                    # 2. Detect Anomalies
                    detector = AnomalyDetector()
                    df = detector.train_and_predict(df)
                    
                    st.session_state['audit_data'] = df
                    st.success("ë¶„ì„ ì™„ë£Œ! ìš°ì¸¡ ëŒ€ì‹œë³´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        with col2:
            if 'audit_data' in st.session_state:
                df = st.session_state['audit_data']
                anomalies = df[df['is_anomaly_detected']]
                
                # Metrics
                m1, m2, m3, m4 = st.columns(4)
                m1.metric("ì´ ê²°ì œ ê±´ìˆ˜", f"{len(df):,}ê±´")
                m2.metric("ì´ ê²°ì œ ê¸ˆì•¡", f"{df['amount'].sum()//100000000}ì–µì›")
                m3.metric("ì´ìƒ ì§•í›„ íƒì§€", f"{len(anomalies):,}ê±´", delta="Risk", delta_color="inverse")
                m4.metric("ì´ìƒ ë¹„ìœ¨", f"{len(anomalies)/len(df)*100:.2f}%")
                
                # Chart
                st.markdown("### 2. ì´ìƒ íƒì§€ ì‹œê°í™”")
                fig = px.scatter(
                    df, 
                    x="transaction_time", 
                    y="amount", 
                    color="is_anomaly_detected",
                    color_discrete_map={True: '#EB008B', False: '#002060'}, # Kiwoom Colors
                    hover_data=['merchant_name', 'employee_name', 'anomaly_type'],
                    title="ì‹œê°„ëŒ€ë³„ ë²•ì¸ì¹´ë“œ ê²°ì œ ê¸ˆì•¡ ë¶„í¬ (Red: ì´ìƒ ì§•í›„)",
                    opacity=0.6,
                    height=500
                )
                fig.update_layout(plot_bgcolor='white')
                st.plotly_chart(fig, use_container_width=True)
                
                # Detail Analysis & Report Generation
                st.markdown("### 3. AI ê°ì‚¬ ë¦¬í¬íŠ¸ ìƒì„±")
                st.write("íƒì§€ëœ ì´ìƒ ì§•í›„ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ AI ê°ì‚¬ ì¡°ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                
                # Filter only anomalies for selection
                anomaly_options = anomalies.sort_values('amount', ascending=False).head(20)
                selected_idx = st.selectbox(
                    "ë¶„ì„í•  ì´ìƒ ê±°ë˜ ì„ íƒ (Top 20 Risk Items):",
                    options=anomaly_options.index,
                    format_func=lambda x: f"[{anomaly_options.loc[x, 'anomaly_type']}] {anomaly_options.loc[x, 'merchant_name']} - {anomaly_options.loc[x, 'amount']:,}ì› ({anomaly_options.loc[x, 'employee_name']})"
                )
                
                if st.button("ğŸ“ AI ê°ì‚¬ ì¡°ì„œ ì‘ì„± (Generate Report)", key="btn_report"):
                    row = df.loc[selected_idx]
                    report = GenAISimulator.generate_audit_report(row)
                    
                    st.markdown('<div class="perplexity-report-container">', unsafe_allow_html=True)
                    st.markdown(report)
                    st.markdown('</div>', unsafe_allow_html=True)

    # -------------------------------------------------------------------------
    # TAB 2: External Threat Intelligence
    # -------------------------------------------------------------------------
    with tab2:
        st.subheader("ğŸŒ ì™¸ë¶€ ê¸ˆìœµ ìœ„í˜‘ ì •ë³´ ìˆ˜ì§‘ ë° ë¶„ì„")
        st.markdown("""
        **Perplexity AI**ë¥¼ í™œìš©í•˜ì—¬ ìµœì‹  ê¸ˆìœµê¶Œ ìœ„í˜‘ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ì‹¬ì¸µ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """)
        
        if st.button("ğŸ” ìµœì‹  ê¸ˆìœµê¶Œ ìœ„í˜‘ ì •ë³´ ìˆ˜ì§‘ (Hybrid)", key="btn_news_auto"):
            with st.spinner("ê¸ˆìœµìœ„ì›íšŒ/ê¸ˆê°ì› ê³µì‹ ìë£Œ ë° Perplexity AI ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
                crawler = NewsCrawler()
                # Pass API key to get_all_news
                news_results = crawler.get_all_news(pplx_api_key if pplx_api_key else None)
                st.session_state['news_results'] = news_results
            st.success(f"ì´ {len(news_results)}ê±´ì˜ ì¤‘ìš” ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ! (FSC/FSS + AI Search)")
            
            if not pplx_api_key:
                st.warning("âš ï¸ Perplexity API Keyê°€ ì…ë ¥ë˜ì§€ ì•Šì•„ ì¼ë°˜ í¬ë¡¤ë§ ëª¨ë“œë¡œ ë™ì‘í–ˆìŠµë‹ˆë‹¤. ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ìœ„í•´ Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            
        if 'news_results' in st.session_state:
            news_list = st.session_state['news_results']
            
            for i, news in enumerate(news_list):
                # Badge Color
                source_lower = news.get('source', '').lower()
                badge_class = "badge-naver"
                card_class = "source-naver"
                
                if "google" in source_lower:
                    badge_class = "badge-google"
                    card_class = "source-google"
                elif "fss" in source_lower or "ê¸ˆê°ì›" in source_lower:
                    badge_class = "badge-fss"
                    card_class = "source-fss"
                elif "fsc" in source_lower or "ê¸ˆìœµìœ„" in source_lower:
                    badge_class = "badge-fsc"
                    card_class = "source-fsc"
                
                with st.container():
                    st.markdown(f"""
                    <div class="news-card {card_class}">
                        <span class="news-badge {badge_class}">{news.get('source', 'Unknown')}</span>
                        <h4 style="margin: 5px 0;">{news.get('title', 'No Title')}</h4>
                        <p style="color: #666; font-size: 0.9rem; margin-bottom: 10px;">{news.get('summary', '')}</p>
                        <a href="{news.get('link', '#')}" target="_blank" style="text-decoration: none; color: #002060; font-weight: bold; font-size: 0.85rem;">ğŸ”— ì›ë¬¸ ë³´ê¸°</a>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # AI Analysis Button for each news item
                    if st.button(f"ğŸ¤– AI ì‹¬ì¸µ ë¶„ì„ (Deep Analysis)", key=f"btn_analyze_{i}"):
                        if not pplx_api_key:
                            st.warning("âš ï¸ Perplexity API Keyë¥¼ ì…ë ¥í•˜ë©´ ë” ì •í™•í•œ ì‹¬ì¸µ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. (í˜„ì¬ëŠ” í…œí”Œë¦¿ ì‚¬ìš©)")
                            # Fallback to Template
                            analysis_result = GenAISimulator.generate_threat_analysis(news['title'])
                            st.markdown(analysis_result)
                        else:
                            with st.spinner("Perplexity AIê°€ í•´ë‹¹ ì‚¬ê±´ì„ ì •ë°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                                # Use Perplexity API for Analysis
                                analysis_result = GenAISimulator.generate_analysis_with_perplexity(pplx_api_key, news)
                                
                                # Display in a styled container
                                st.markdown('<div class="perplexity-report-container">', unsafe_allow_html=True)
                                st.markdown(analysis_result, unsafe_allow_html=True)
                                st.markdown('</div>', unsafe_allow_html=True)

if __name__ == "__main__":
    main()
